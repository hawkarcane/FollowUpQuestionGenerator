{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 2 : Follow-up Question Generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#imports\n",
    "import pandas as pd\n",
    "import csv\n",
    "import json\n",
    "import pickle\n",
    "import spacy\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.python.layers.core import Dense\n",
    "from tensorflow.contrib.seq2seq.python.ops import beam_search_ops\n",
    "import time\n",
    "import model\n",
    "import os\n",
    "from random import randint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('your_csv_file.csv', 'r') as file:  \n",
    "    reader = csv.reader(file)\n",
    "    result = list(reader) #Putting them all in a list\n",
    "\n",
    "paragraph_list = []\n",
    "for i in range(len(result)-1):\n",
    "    paragraph1 = ' '.join(map(str, result[i+1][1:3]))\n",
    "    paragraph_list.append(paragraph1)\n",
    "\n",
    "followupque_list = []\n",
    "for j in range(len(result)-1):\n",
    "    paragraph2 = ''.join(map(str, result[j+1][3]))\n",
    "    followupque_list.append(paragraph2)\n",
    "\n",
    "df = pd.DataFrame(data={\"paragraph\": paragraph_list, \"followup question\": followupque_list})\n",
    "df.to_csv(\"./newdata.csv\", sep=',',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Pickle up the extracted lists of questions/answers pairs\n",
    "def savepickle(data, filename):\n",
    "    \"\"\"Saves the data into pickle format\"\"\"\n",
    "    save_documents = open(filename +'.pickle', 'wb')\n",
    "    pickle.dump(data, save_documents)\n",
    "    save_documents.close()\n",
    "    \n",
    "savepickle(paragraph_list, 'train_paragraphs')\n",
    "savepickle(followupque_list, 'train_questions')\n",
    "savepickle(paragraph_list, 'dev_paragraphs')\n",
    "savepickle(followupque_list, 'dev_questions')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load up spacy model and import stop words\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "from spacy.lang.en import STOP_WORDS\n",
    "for word in STOP_WORDS:\n",
    "    lexeme = nlp.vocab[word]\n",
    "    lexeme.is_stop = True\n",
    "    \n",
    "def loadpickle(data_filepath):\n",
    "    #Loads up the pickled dataset for further parsing and preprocessing\n",
    "    documents_f = open(data_filepath+'.pickle', 'rb')\n",
    "    data = pickle.load(documents_f)\n",
    "    documents_f.close()\n",
    "    \n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocessData(data, remove_stopwords=True, replace_entities=False):\n",
    "    parsed_data = []\n",
    "    for index in range(len(data)):\n",
    "        text = data[index]\n",
    "        \n",
    "        if replace_entities:\n",
    "            spacy_text = nlp(text)\n",
    "            text_ents = [(str(ent), str(ent.label_)) for ent in spacy_text.ents]\n",
    "            \n",
    "            text = text.lower()\n",
    "            for ent in text_ents:\n",
    "                replacee = str(ent[0].lower())\n",
    "                replacer = str(ent[1])\n",
    "                try:\n",
    "                    text = text.replace(replacee, replacer)\n",
    "                except:\n",
    "                    pass\n",
    "        else:\n",
    "            text = text.lower()\n",
    "            \n",
    "        text = nlp(text)\n",
    "        if remove_stopwords:\n",
    "            text = [str(token.orth_) for token in text \n",
    "                    if not token.is_stop and not token.is_punct]\n",
    "            text = ' '.join(text)\n",
    "        else:\n",
    "            text = [str(token.orth_) for token in text if not token.is_punct]\n",
    "            text = ' '.join(text)\n",
    "            \n",
    "        parsed_data.append(text)\n",
    "        \n",
    "        if index % 100 == 0 and index > 0:\n",
    "            print('Preprocessing {}/{}'.format(index, len(data)))\n",
    "            \n",
    "        if index % 1000 == 0 and index > 0:\n",
    "            print('Pickling progress so far.')\n",
    "            savepickle(parsed_data, 'parsed_data')\n",
    "         \n",
    "        if index % 2000 == 0:\n",
    "            try:\n",
    "                print(text)\n",
    "            except:\n",
    "                pass\n",
    "\n",
    "    return parsed_data\n",
    "    \n",
    "def loadEmbeddings(embeddings_index, filepath):\n",
    "    print('Loading Conceptnet Numberbatch word embeddings')\n",
    "    with open(filepath, encoding='utf-8') as f:\n",
    "        for line in f:\n",
    "            values = line.split(' ')\n",
    "            word = values[0]\n",
    "            embedding = np.asarray(values[1:], dtype='float32')\n",
    "            embeddings_index[word] = embedding\n",
    "    \n",
    "    print('Word embeddings:', len(embeddings_index))\n",
    "    \n",
    "def countWordFreq(word_frequency, data):\n",
    "    for text in data:\n",
    "        for token in text.split():\n",
    "            if token not in word_frequency:\n",
    "                word_frequency[token] = 1\n",
    "            else:\n",
    "                word_frequency[token] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def conversionDicts(word_frequency, embeddings_index, threshold=10):\n",
    "    missing_words = 0\n",
    "    \n",
    "    for token, freq in word_frequency.items():\n",
    "        if freq > threshold:\n",
    "            if token not in embeddings_index:\n",
    "                missing_words += 1\n",
    "                \n",
    "    missing_ratio = round(missing_words/len(word_frequency), 4) * 100\n",
    "    print('Number of words missing from Conceptnet Numberbatch:', missing_words)\n",
    "    print('Percent of words that are missing from vocabulary: ', missing_ratio, '%')\n",
    "\n",
    "    #Dictionary to convert words to integers\n",
    "    print('Creating vocab_to_int dictionary')\n",
    "    vocab2int = {}\n",
    "    \n",
    "    value = 0\n",
    "    for token, freq in word_frequency.items():\n",
    "        if freq >= threshold or token in embeddings_index:\n",
    "            vocab2int[token] = value\n",
    "            value += 1\n",
    "    \n",
    "    # Special tokens that will be added to our vocab. Those tokens will guide the sequence to sequence model\n",
    "    codes = ['<UNK>', '<PAD>', '<EOS>', '<GO>']   \n",
    "    \n",
    "    print('Adding special tokens to vocab_to_int dictionary.')\n",
    "    for code in codes:\n",
    "        vocab2int[code] = len(vocab2int)\n",
    "    \n",
    "    #Dictionary to convert integers to words\n",
    "    print('Creating int_to_vocab dictionary.')\n",
    "    int2vocab = {}\n",
    "    for token, index in vocab2int.items():\n",
    "        int2vocab[index] = token\n",
    "    \n",
    "    usage_ratio = round(len(vocab2int) / len(word_frequency), 4) * 100\n",
    "    print(\"Total number of unique words:\", len(word_frequency))\n",
    "    print(\"Number of words we will use:\", len(vocab2int))\n",
    "    print(\"Percent of words we will use: {}%\".format(usage_ratio))\n",
    "    \n",
    "    return vocab2int, int2vocab\n",
    "\n",
    "def embeddingMatrix(vocab2int, embeddings_index, embedding_dimensions=300):\n",
    "    num_words = len(vocab2int)\n",
    "    #Creating a default matrix with all values set to zero and fill it out\n",
    "    print('Creating word embedding matrix with all the tokens and their corresponding vectors.')\n",
    "    word_embedding_matrix = np.zeros((num_words, embedding_dimensions), dtype=np.float32)\n",
    "    for token, index in vocab2int.items():\n",
    "        if token in embeddings_index:\n",
    "            word_embedding_matrix[index] = embeddings_index[token]\n",
    "        else:\n",
    "            new_embedding = np.array(np.random.uniform(-1.0, 1.0, embedding_dimensions))\n",
    "            word_embedding_matrix[index] = new_embedding\n",
    "            \n",
    "    return word_embedding_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convData2Int(data, vocab2int, word_count, unk_count, eos=True):\n",
    "    converted_data = []\n",
    "    for text in data:\n",
    "        converted_text = []\n",
    "        for token in text.split():\n",
    "            word_count += 1\n",
    "            if token in vocab2int:\n",
    "                converted_text.append(vocab2int[token])\n",
    "            else:\n",
    "                converted_text.append(vocab2int['<UNK>'])\n",
    "                unk_count += 1\n",
    "        if eos:\n",
    "            converted_text.append(vocab2int['<EOS>'])\n",
    "            \n",
    "        converted_data.append(converted_text)\n",
    "    \n",
    "    assert len(converted_data) == len(data)\n",
    "    return converted_data, word_count, unk_count\n",
    "\n",
    "def summary(data):\n",
    "    summary = []\n",
    "    for text in data:\n",
    "        summary.append(len(text))\n",
    "    return pd.DataFrame(summary, columns=['counts'])\n",
    "\n",
    "def unkCounter(data, vocab2int):\n",
    "    unk_count = 0\n",
    "    for token in data:\n",
    "        if token == vocab2int['<UNK>']:\n",
    "            unk_count += 1\n",
    "    return unk_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remWrongLenData(coverted_inputs, converted_targets, vocab2int,\n",
    "                             start_inputs_length, max_inputs_length, max_targets_length, \n",
    "                             min_inputs_length=10, min_targets_lengths=5,\n",
    "                             unk_inputs_limit=1, unk_targets_limit=0):\n",
    "    sorted_inputs = []\n",
    "    sorted_targets = []\n",
    "    \n",
    "    for length in range(start_inputs_length, max_inputs_length): \n",
    "        for index, words in enumerate(converted_targets):\n",
    "            if (len(converted_targets[index]) >= min_targets_lengths and\n",
    "                len(converted_targets[index]) <= max_targets_length and\n",
    "                len(coverted_inputs[index]) >= min_inputs_length and\n",
    "                unkCounter(converted_targets[index], vocab2int) <= unk_targets_limit and\n",
    "                unkCounter(coverted_inputs[index], vocab2int) <= unk_inputs_limit and\n",
    "                length == len(coverted_inputs[index])\n",
    "               ):\n",
    "                sorted_targets.append(converted_targets[index])\n",
    "                sorted_inputs.append(coverted_inputs[index])\n",
    "        \n",
    "    #Ensuring the length of sorted paragraph and questions match\n",
    "    assert len(sorted_inputs) == len(sorted_targets)\n",
    "    print('Got {} inputs/targets pairs!'.format(len(sorted_inputs)))\n",
    "    \n",
    "    return sorted_inputs, sorted_targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 1086 question/answer pairs.\n"
     ]
    }
   ],
   "source": [
    "#Loading the dataset\n",
    "data_inputs = loadpickle('train_squad_paragraphs')\n",
    "data_targets = loadpickle('train_squad_questions') \n",
    "assert len(data_targets) == len(data_inputs)\n",
    "print('Loaded {} question/answer pairs.'.format(len(data_inputs)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#If parsed dataset is found, we try loading it.\n",
    "try:\n",
    "    parsed_inputs = loadpickle('parsed_inputs')\n",
    "    parsed_targets = loadpickle('parsed_targets')\n",
    "except:\n",
    "    print('Preprocessing inputs, this may take a while...')\n",
    "    parsed_inputs = preprocessData(data_inputs, remove_stopwords=True,\n",
    "                                    replace_entities=True)\n",
    "    savepickle(parsed_inputs, 'parsed_inputs')\n",
    "    print('Preprocessing targets, this may take a while...')\n",
    "    parsed_targets = preprocessData(data_targets, remove_stopwords=False,\n",
    "                                     replace_entities=True)\n",
    "    savepickle(parsed_targets, 'parsed_targets')\n",
    "    \n",
    "    assert len(parsed_inputs) == len(parsed_targets)\n",
    "    print('Loaded up {} parsed inputs/targets pairs'.format(len(parsed_inputs)))\n",
    "\n",
    "if 1:\n",
    "    savepickle(parsed_inputs, 'parsed_inputs')\n",
    "    savepickle(parsed_targets, 'parsed_targets')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Conceptnet Numberbatch word embeddings\n",
      "Word embeddings: 417195\n",
      "Removing token which frequency in the corpus is under specified threshold\n",
      "Number of words missing from Conceptnet Numberbatch: 17\n",
      "Percent of words that are missing from vocabulary:  0.44999999999999996 %\n",
      "Creating vocab_to_int dictionary\n",
      "Adding special tokens to vocab_to_int dictionary.\n",
      "Creating int_to_vocab dictionary.\n",
      "Total number of unique words: 3751\n",
      "Number of words we will use: 3244\n",
      "Percent of words we will use: 86.48%\n",
      "Creating word embedding matrix with all the tokens and their corresponding vectors.\n"
     ]
    }
   ],
   "source": [
    "#Load Numberbatch word embeddings\n",
    "filepath = 'numberbatch-en-17.06.txt'\n",
    "embeddings_index = {}\n",
    "loadEmbeddings(embeddings_index, filepath)\n",
    "\n",
    "#Calculate word frequency\n",
    "word_frequency = {}\n",
    "countWordFreq(word_frequency, parsed_targets)\n",
    "countWordFreq(word_frequency, parsed_inputs)\n",
    "\n",
    "#Get the usable only tokens and their integer conversion\n",
    "vocab2int, int2vocab = conversionDicts(word_frequency, embeddings_index)\n",
    "savepickle(vocab2int, 'vocab2int')\n",
    "savepickle(int2vocab, 'int2vocab')\n",
    "\n",
    "#Create embedding matrix\n",
    "word_embedding_matrix = embeddingMatrix(vocab2int, embeddings_index)\n",
    "del embeddings_index\n",
    "savepickle(word_embedding_matrix, 'word_embedding_matrix')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converting text to integers\n",
      "Total number of words: 38009\n",
      "Total number of UNKs: 1294\n",
      "Percent of words that are UNK: 3.4000000000000004\n"
     ]
    }
   ],
   "source": [
    "#Convert words to integers and pickle the data\n",
    "word_count = 0\n",
    "unk_count = 0\n",
    "\n",
    "print('Converting text to integers')\n",
    "converted_inputs, word_count, unk_count = convData2Int(parsed_inputs, vocab2int, word_count, unk_count)\n",
    "converted_targets, word_count, unk_count = convData2Int(parsed_targets, vocab2int, word_count,  unk_count)\n",
    "assert len(converted_inputs) == len(converted_targets)\n",
    "\n",
    "unk_percent = round(unk_count/word_count, 4) * 100\n",
    "print('Total number of words:', word_count)\n",
    "print('Total number of UNKs:', unk_count)\n",
    "print('Percent of words that are UNK:', unk_percent)\n",
    "\n",
    "savepickle(converted_inputs, 'converted_inputs')\n",
    "savepickle(converted_targets, 'converted_targets')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inputs:\n",
      "            counts\n",
      "count  1086.000000\n",
      "mean     25.988029\n",
      "std      16.116707\n",
      "min       2.000000\n",
      "25%      15.000000\n",
      "50%      22.000000\n",
      "75%      34.000000\n",
      "max     143.000000\n",
      "##################################################\n",
      "Targets\n",
      "            counts\n",
      "count  1086.000000\n",
      "mean     11.011050\n",
      "std       4.729344\n",
      "min       2.000000\n",
      "25%       8.000000\n",
      "50%      10.000000\n",
      "75%      13.000000\n",
      "max      42.000000\n",
      "Got 763 inputs/targets pairs!\n"
     ]
    }
   ],
   "source": [
    "#Build summary and sort the data to keep only the appropriate length\n",
    "assert len(converted_inputs) == len(converted_targets)\n",
    "\n",
    "summary_inputs = summary(converted_inputs)\n",
    "summary_targets = summary(converted_targets)\n",
    "\n",
    "print('Inputs:')\n",
    "print(summary_inputs.describe())\n",
    "print('#' * 50)\n",
    "print('Targets')\n",
    "print(summary_targets.describe())\n",
    "\n",
    "sorted_inputs, sorted_targets = remWrongLenData(converted_inputs,\n",
    "                                                         converted_targets,\n",
    "                                                         vocab2int,\n",
    "                                                         start_inputs_length=min(summary_inputs.counts),\n",
    "                                                         max_inputs_length=int(np.percentile(summary_inputs.counts, 100)),\n",
    "                                                         max_targets_length=int(np.percentile(summary_targets.counts, 100)),\n",
    "                                                         min_inputs_length=10,\n",
    "                                                         min_targets_lengths=5,\n",
    "                                                         unk_inputs_limit=1,\n",
    "                                                         unk_targets_limit=0)\n",
    "\n",
    "#print('Pickling the final files.')\n",
    "savepickle(sorted_inputs, 'sorted_inputs')\n",
    "savepickle(sorted_targets, 'sorted_targets')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Building the model\n",
    "\n",
    "def model_inputs():\n",
    "    \n",
    "    input_data = tf.placeholder(tf.int32, [None, None], name='input')\n",
    "    targets = tf.placeholder(tf.int32, [None, None], name='targets')\n",
    "    lr = tf.placeholder(tf.float32, name='learning_rate')\n",
    "    keep_prob = tf.placeholder(tf.float32, name='keep_prob')\n",
    "    target_length = tf.placeholder(tf.int32, (None,), name='target_length')\n",
    "    max_target_length = tf.reduce_max(target_length, name='max_dec_len')\n",
    "    input_length = tf.placeholder(tf.int32, (None,), name='input_length')\n",
    "\n",
    "    return input_data, targets, lr, keep_prob, \\\n",
    "           target_length, max_target_length, input_length\n",
    "           \n",
    "def processEncodingInput(target_data, vocab2int, batch_size):\n",
    "    ending = tf.strided_slice(target_data, [0, 0], [batch_size, -1], [1, 1])\n",
    "    dec_input = tf.concat([tf.fill([batch_size, 1], vocab2int['<GO>']), ending], 1)\n",
    "\n",
    "    return dec_input\n",
    "\n",
    "def encodingLayer(rnn_size, sequence_length, num_layers, rnn_inputs, keep_prob):\n",
    "    #Create the encoding layer\n",
    "    \n",
    "    for layer in range(num_layers):\n",
    "        with tf.variable_scope('encoder_{}'.format(layer)):\n",
    "            cell_fw = tf.contrib.rnn.LSTMCell(rnn_size,\n",
    "                                              initializer=tf.random_uniform_initializer(-0.1, 0.1, seed=2))\n",
    "            cell_fw = tf.contrib.rnn.DropoutWrapper(cell_fw, \n",
    "                                                    input_keep_prob=keep_prob)\n",
    "\n",
    "            cell_bw = tf.contrib.rnn.LSTMCell(rnn_size,\n",
    "                                              initializer=tf.random_uniform_initializer(-0.1, 0.1, seed=2))\n",
    "            cell_bw = tf.contrib.rnn.DropoutWrapper(cell_bw, \n",
    "                                                    input_keep_prob=keep_prob)\n",
    "\n",
    "            enc_output, enc_state = tf.nn.bidirectional_dynamic_rnn(cell_fw, \n",
    "                                                                    cell_bw, \n",
    "                                                                    rnn_inputs,\n",
    "                                                                    sequence_length,\n",
    "                                                                    dtype=tf.float32)\n",
    "    #Join outputs since we are using a bidirectional RNN\n",
    "    enc_output = tf.concat(enc_output, 2)\n",
    "    \n",
    "    return enc_output, enc_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decodingAttentionTraining(rnn_size, enc_output, enc_state, input_length, \n",
    "                                dec_cell, batch_size):\n",
    "    \n",
    "    attn_mech_training = tf.contrib.seq2seq.BahdanauAttention(rnn_size,\n",
    "                                                              enc_output,\n",
    "                                                              input_length,\n",
    "                                                              normalize=False,\n",
    "                                                              name='BahdanauAttention')\n",
    "    dec_cell_training = tf.contrib.seq2seq.AttentionWrapper(cell=dec_cell,\n",
    "                                                            attention_mechanism=attn_mech_training,\n",
    "                                                            attention_layer_size=rnn_size)\n",
    "    initial_state_training = dec_cell_training.zero_state(batch_size, tf.float32)\n",
    "    initial_state_training = initial_state_training.clone(cell_state=enc_state[0])\n",
    "    \n",
    "    return dec_cell_training, initial_state_training\n",
    "\n",
    "def decodingLayerTraining(dec_embed_input, target_length, dec_cell, initial_state, \n",
    "                            output_layer, vocab_size, max_target_length):\n",
    "    \n",
    "    training_helper = tf.contrib.seq2seq.TrainingHelper(inputs=dec_embed_input,\n",
    "                                                        sequence_length=target_length,\n",
    "                                                        time_major=False)\n",
    "    training_decoder = tf.contrib.seq2seq.BasicDecoder(dec_cell,\n",
    "                                                       training_helper,\n",
    "                                                       initial_state,\n",
    "                                                       output_layer) \n",
    "    training_logits = tf.contrib.seq2seq.dynamic_decode(training_decoder,\n",
    "                                                           output_time_major=False,\n",
    "                                                           impute_finished=True,\n",
    "                                                           maximum_iterations=max_target_length)\n",
    "    return training_logits[0]\n",
    "\n",
    "def decodingAttentionInference(enc_output, enc_state, input_length, rnn_size, dec_cell,\n",
    "                                batch_size, beam_width):\n",
    "    tiled_encoder_outputs = tf.contrib.seq2seq.tile_batch(enc_output, multiplier=beam_width)\n",
    "    tiled_encoder_final_state = tf.contrib.seq2seq.tile_batch(enc_state[0], multiplier=beam_width)\n",
    "    tiled_sequence_length = tf.contrib.seq2seq.tile_batch(input_length, multiplier=beam_width)\n",
    "    \n",
    "    attn_mech_sample = tf.contrib.seq2seq.BahdanauAttention(num_units=rnn_size,\n",
    "                                                            memory=tiled_encoder_outputs,\n",
    "                                                            memory_sequence_length=tiled_sequence_length)\n",
    "    dec_cell_inference = tf.contrib.seq2seq.AttentionWrapper(cell=dec_cell,\n",
    "                                                             attention_mechanism=attn_mech_sample,\n",
    "                                                             attention_layer_size=rnn_size)\n",
    "    decoder_initial_state_inference = dec_cell_inference.zero_state(dtype=tf.float32, \n",
    "                                                                   batch_size=batch_size*beam_width)\n",
    "    decoder_initial_state_inference = decoder_initial_state_inference.clone(cell_state=tiled_encoder_final_state)\n",
    "    \n",
    "    return dec_cell_inference, decoder_initial_state_inference\n",
    "\n",
    "def decodingLayerInference(embeddings, start_token, end_token, dec_cell, initial_state, \n",
    "                             output_layer, max_target_length, batch_size, beam_width):\n",
    "\n",
    "    start_tokens = tf.tile(tf.constant([start_token], dtype=tf.int32), \n",
    "                           [batch_size], \n",
    "                           name='start_tokens')    \n",
    "    inference_decoder = tf.contrib.seq2seq.BeamSearchDecoder(cell=dec_cell,\n",
    "                                                             embedding=embeddings,\n",
    "                                                             start_tokens=start_tokens,\n",
    "                                                             end_token=end_token,\n",
    "                                                             initial_state=initial_state,\n",
    "                                                             beam_width=beam_width,\n",
    "                                                             output_layer=output_layer)\n",
    "    inference_logits = tf.contrib.seq2seq.dynamic_decode(inference_decoder,\n",
    "                                                         output_time_major=False,\n",
    "                                                         impute_finished=False,\n",
    "                                                         maximum_iterations=max_target_length)\n",
    "    \n",
    "    return inference_logits[0]\n",
    "\n",
    "def decodingLayer(dec_embed_input, embeddings, enc_output, enc_state, vocab_size, \n",
    "                   input_length, target_length, max_target_length, rnn_size, \n",
    "                   vocab2int, keep_prob, batch_size, num_layers, beam_width):\n",
    "\n",
    "    for layer in range(num_layers):\n",
    "        with tf.variable_scope('decoder_{}'.format(layer)):\n",
    "            lstm = tf.contrib.rnn.LSTMCell(rnn_size,\n",
    "                                           initializer=tf.random_uniform_initializer(-0.1, 0.1, seed=2))\n",
    "            dec_cell = tf.contrib.rnn.DropoutWrapper(lstm, \n",
    "                                                     input_keep_prob = keep_prob)\n",
    "    \n",
    "    output_layer = Dense(vocab_size,\n",
    "                         kernel_initializer=tf.truncated_normal_initializer(mean = 0.0, stddev=0.1))\n",
    "    #Training decode using standard decoder  \n",
    "    dec_cell_training, initial_state_training = decodingAttentionTraining(rnn_size, \n",
    "                                                                            enc_output, \n",
    "                                                                            enc_state, \n",
    "                                                                            input_length, \n",
    "                                                                            dec_cell, \n",
    "                                                                            batch_size)\n",
    "    \n",
    "    with tf.variable_scope(\"decode\"):\n",
    "        training_logits = decodingLayerTraining(dec_embed_input, \n",
    "                                                  target_length, \n",
    "                                                  dec_cell_training, \n",
    "                                                  initial_state_training,\n",
    "                                                  output_layer,\n",
    "                                                  vocab_size, \n",
    "                                                  max_target_length)\n",
    "    #Inference decoding using beam search\n",
    "    dec_cell_inference, decoder_init_state_inference = decodingAttentionInference(enc_output, \n",
    "                                                                                    enc_state, \n",
    "                                                                                    input_length, \n",
    "                                                                                    rnn_size, \n",
    "                                                                                    dec_cell,\n",
    "                                                                                    batch_size, \n",
    "                                                                                    beam_width)\n",
    "        \n",
    "    with tf.variable_scope(\"decode\", reuse=True):\n",
    "        inference_logits = decodingLayerInference(embeddings,  \n",
    "                                                    vocab2int['<GO>'], \n",
    "                                                    vocab2int['<EOS>'],\n",
    "                                                    dec_cell_inference, \n",
    "                                                    decoder_init_state_inference, \n",
    "                                                    output_layer,\n",
    "                                                    max_target_length,\n",
    "                                                    batch_size,\n",
    "                                                    beam_width)\n",
    "\n",
    "    return training_logits, inference_logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def padTextBatch(data_batch, vocab2int):\n",
    "    max_text = max([len(text) for text in data_batch])\n",
    "    return [text + [vocab2int['<PAD>']] * (max_text - len(text)) for text in data_batch]\n",
    "\n",
    "def get_batches(targets, inputs, vocab2int, batch_size):\n",
    "    for batch_i in range(0, len(inputs)//batch_size):\n",
    "        start_i = batch_i * batch_size\n",
    "        targets_batch = targets[start_i:start_i + batch_size]\n",
    "        inputs_batch = inputs[start_i:start_i + batch_size]\n",
    "        pad_targets_batch = np.array(padTextBatch(targets_batch, vocab2int))\n",
    "        pad_inputs_batch = np.array(padTextBatch(inputs_batch, vocab2int))\n",
    "        pad_targets_lenghts = []\n",
    "        for target in pad_targets_batch:\n",
    "            pad_targets_lenghts.append(len(target))\n",
    "        pad_inputs_lenghts = []\n",
    "        for text in pad_inputs_batch:\n",
    "            pad_inputs_lenghts.append(len(text))\n",
    "        yield pad_targets_batch, pad_inputs_batch, pad_targets_lenghts, pad_inputs_lenghts     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading and preparing data for training...\n"
     ]
    }
   ],
   "source": [
    "#Load data\n",
    "print('Loading and preparing data for training...')\n",
    "enc_inputs = loadpickle('sorted_inputs')\n",
    "dec_targets = loadpickle('sorted_targets')\n",
    "vocab2int = loadpickle('vocab2int')\n",
    "int2vocab = loadpickle('int2vocab')\n",
    "word_embedding_matrix = loadpickle('word_embedding_matrix')\n",
    "assert len(enc_inputs) == len(dec_targets)\n",
    "assert len(vocab2int) == len(int2vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building graph\n",
      "Graph is built.\n"
     ]
    }
   ],
   "source": [
    "#Setting the parameters\n",
    "epochs = 100\n",
    "batch_size = 32\n",
    "rnn_size = 256\n",
    "num_layers = 2\n",
    "learning_rate = 0.005\n",
    "keep_probability = 0.8     \n",
    "beam_width = 20\n",
    "\n",
    "print('Building graph')\n",
    "# Build the graph\n",
    "train_graph = tf.Graph()\n",
    "# Set the graph to default to ensure that it is ready for training\n",
    "with train_graph.as_default():\n",
    "    \n",
    "    # Load the model inputs    \n",
    "    input_data, targets, lr, keep_prob, target_length, max_target_length, input_length = model.model_inputs()\n",
    "\n",
    "    # Create the training and inference logits\n",
    "    training_logits, inference_logits = model.seq2seq_model(tf.reverse(input_data, [-1]),\n",
    "                                                            targets, \n",
    "                                                            keep_prob,   \n",
    "                                                            input_length,\n",
    "                                                            target_length,\n",
    "                                                            max_target_length,\n",
    "                                                            len(vocab2int)+1,\n",
    "                                                            rnn_size, \n",
    "                                                            num_layers, \n",
    "                                                            vocab2int,\n",
    "                                                            word_embedding_matrix,\n",
    "                                                            batch_size,\n",
    "                                                            beam_width)\n",
    "    \n",
    "    # Create tensors for the training logits and inference logits\n",
    "    training_logits = tf.identity(training_logits.rnn_output, 'logits')\n",
    "    inference_logits = tf.identity(inference_logits.predicted_ids, name='predictions')\n",
    "    \n",
    "    # Create the weights for sequence_loss\n",
    "    masks = tf.sequence_mask(target_length, max_target_length, dtype=tf.float32, name='masks')\n",
    "\n",
    "    with tf.name_scope(\"optimization\"):\n",
    "        # Loss function\n",
    "        cost = tf.contrib.seq2seq.sequence_loss(\n",
    "            training_logits,\n",
    "            targets,\n",
    "            masks)\n",
    "\n",
    "        # Optimizer\n",
    "        optimizer = tf.train.AdamOptimizer(learning_rate)\n",
    "\n",
    "        # Gradient Clipping\n",
    "        gradients = optimizer.compute_gradients(cost)\n",
    "        capped_gradients = [(tf.clip_by_value(grad, -5., 5.), var) for grad, var in gradients if grad is not None]\n",
    "        train_op = optimizer.apply_gradients(capped_gradients)\n",
    "print(\"Graph is built.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing session and training\n",
      "Epoch   1/100 Batch    0/23 - Loss:  0.355, Seconds: 35.93\n",
      "Mean loss for this update: 4.721\n",
      "Mean loss for this update: 2.347\n",
      "Mean loss for this update: 2.786\n",
      "Epoch   2/100 Batch    0/23 - Loss:  0.135, Seconds: 5.11\n",
      "Mean loss for this update: 2.781\n",
      "Mean loss for this update: 2.066\n",
      "Mean loss for this update: 2.523\n",
      "Epoch   3/100 Batch    0/23 - Loss:  0.128, Seconds: 4.96\n",
      "Mean loss for this update: 2.666\n",
      "Mean loss for this update: 1.984\n",
      "Mean loss for this update: 2.401\n",
      "Epoch   4/100 Batch    0/23 - Loss:  0.119, Seconds: 5.40\n",
      "Mean loss for this update: 2.524\n",
      "Mean loss for this update: 1.889\n",
      "Mean loss for this update: 2.304\n",
      "Epoch   5/100 Batch    0/23 - Loss:  0.114, Seconds: 5.16\n",
      "Mean loss for this update: 2.416\n",
      "Mean loss for this update: 1.826\n",
      "Mean loss for this update: 2.186\n",
      "Epoch   6/100 Batch    0/23 - Loss:  0.110, Seconds: 5.04\n",
      "Mean loss for this update: 2.291\n",
      "Mean loss for this update: 1.749\n",
      "Mean loss for this update: 2.114\n",
      "Epoch   7/100 Batch    0/23 - Loss:  0.099, Seconds: 4.92\n",
      "Mean loss for this update: 2.149\n",
      "Mean loss for this update: 1.692\n",
      "Mean loss for this update: 1.979\n",
      "Epoch   8/100 Batch    0/23 - Loss:  0.096, Seconds: 5.24\n",
      "Mean loss for this update: 2.022\n",
      "Mean loss for this update: 1.604\n",
      "Mean loss for this update: 1.918\n",
      "Epoch   9/100 Batch    0/23 - Loss:  0.084, Seconds: 5.20\n",
      "Mean loss for this update: 1.875\n",
      "Mean loss for this update: 1.534\n",
      "Mean loss for this update: 1.778\n",
      "Epoch  10/100 Batch    0/23 - Loss:  0.082, Seconds: 5.34\n",
      "Mean loss for this update: 1.785\n",
      "Mean loss for this update: 1.476\n",
      "Mean loss for this update: 1.714\n",
      "Epoch  11/100 Batch    0/23 - Loss:  0.079, Seconds: 5.50\n",
      "Mean loss for this update: 1.696\n",
      "Mean loss for this update: 1.418\n",
      "Mean loss for this update: 1.604\n",
      "Epoch  12/100 Batch    0/23 - Loss:  0.073, Seconds: 5.34\n",
      "Mean loss for this update: 1.58\n",
      "Mean loss for this update: 1.303\n",
      "Mean loss for this update: 1.502\n",
      "Epoch  13/100 Batch    0/23 - Loss:  0.067, Seconds: 5.38\n",
      "Mean loss for this update: 1.448\n",
      "Mean loss for this update: 1.193\n",
      "Mean loss for this update: 1.327\n",
      "Epoch  14/100 Batch    0/23 - Loss:  0.061, Seconds: 5.54\n",
      "Mean loss for this update: 1.343\n",
      "Mean loss for this update: 1.11\n",
      "Mean loss for this update: 1.234\n",
      "Epoch  15/100 Batch    0/23 - Loss:  0.055, Seconds: 5.48\n",
      "Mean loss for this update: 1.233\n",
      "Mean loss for this update: 1.071\n",
      "Mean loss for this update: 1.203\n",
      "Epoch  16/100 Batch    0/23 - Loss:  0.051, Seconds: 4.93\n",
      "Mean loss for this update: 1.133\n",
      "Mean loss for this update: 1.005\n",
      "Mean loss for this update: 1.073\n",
      "Epoch  17/100 Batch    0/23 - Loss:  0.048, Seconds: 5.74\n",
      "Mean loss for this update: 1.061\n",
      "Mean loss for this update: 0.928\n",
      "Mean loss for this update: 0.993\n",
      "Epoch  18/100 Batch    0/23 - Loss:  0.044, Seconds: 5.34\n",
      "Mean loss for this update: 0.993\n",
      "Mean loss for this update: 0.877\n",
      "Mean loss for this update: 0.946\n",
      "Epoch  19/100 Batch    0/23 - Loss:  0.046, Seconds: 5.48\n",
      "Mean loss for this update: 0.946\n",
      "Mean loss for this update: 0.844\n",
      "Mean loss for this update: 0.888\n",
      "Epoch  20/100 Batch    0/23 - Loss:  0.048, Seconds: 4.92\n",
      "Mean loss for this update: 0.902\n",
      "Mean loss for this update: 0.76\n",
      "Mean loss for this update: 0.784\n",
      "Epoch  21/100 Batch    0/23 - Loss:  0.035, Seconds: 5.21\n",
      "Mean loss for this update: 0.788\n",
      "Mean loss for this update: 0.687\n",
      "Mean loss for this update: 0.74\n",
      "Epoch  22/100 Batch    0/23 - Loss:  0.032, Seconds: 5.35\n",
      "Mean loss for this update: 0.701\n",
      "Mean loss for this update: 0.683\n",
      "Mean loss for this update: 0.711\n",
      "Epoch  23/100 Batch    0/23 - Loss:  0.032, Seconds: 5.15\n",
      "Mean loss for this update: 0.71\n",
      "Mean loss for this update: 0.617\n",
      "Mean loss for this update: 0.671\n",
      "Epoch  24/100 Batch    0/23 - Loss:  0.033, Seconds: 5.48\n",
      "Mean loss for this update: 0.709\n",
      "Mean loss for this update: 0.599\n",
      "Mean loss for this update: 0.68\n",
      "Epoch  25/100 Batch    0/23 - Loss:  0.025, Seconds: 5.11\n",
      "Mean loss for this update: 0.632\n",
      "Mean loss for this update: 0.557\n",
      "Mean loss for this update: 0.607\n",
      "Epoch  26/100 Batch    0/23 - Loss:  0.025, Seconds: 5.35\n",
      "Mean loss for this update: 0.549\n",
      "Mean loss for this update: 0.508\n",
      "Mean loss for this update: 0.542\n",
      "Epoch  27/100 Batch    0/23 - Loss:  0.022, Seconds: 5.25\n",
      "Mean loss for this update: 0.527\n",
      "Mean loss for this update: 0.453\n",
      "Mean loss for this update: 0.526\n",
      "Epoch  28/100 Batch    0/23 - Loss:  0.021, Seconds: 5.27\n",
      "Mean loss for this update: 0.494\n",
      "Mean loss for this update: 0.458\n",
      "Stopping Training since minimum loss is achieved\n"
     ]
    }
   ],
   "source": [
    "#Training the model\n",
    "\n",
    "learning_rate_decay = 0.95\n",
    "min_learning_rate = 0.0005\n",
    "display_step = 23 # Check training loss after every 20 batches\n",
    "stop_early = 0 \n",
    "stop = 3 # If the update loss does not decrease in 3 consecutive update checks, stop training\n",
    "per_epoch = 3 # Make 3 update checks per epoch\n",
    "update_check = (len(enc_inputs)//batch_size//per_epoch)-1\n",
    "\n",
    "update_loss = 0 \n",
    "batch_loss = 0\n",
    "# Record the update losses for saving improvements in the model\n",
    "question_update_loss = [] \n",
    "checkpoint_dir = 'ckpt' \n",
    "checkpoint_path = os.path.join(checkpoint_dir, 'model.ckpt')\n",
    "\n",
    "\n",
    "restore = 0\n",
    "\n",
    "print('Initializing session and training')\n",
    "with tf.Session(graph=train_graph) as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    saver = tf.train.Saver() \n",
    "    \n",
    "    # If we want to continue training a previous session\n",
    "    ckpt = tf.train.get_checkpoint_state(checkpoint_dir)\n",
    "    if ckpt and restore:\n",
    "        print('Restoring old model parameters from %s...' % ckpt.model_checkpoint_path)\n",
    "        saver.restore(sess, ckpt.model_checkpoint_path)\n",
    "    \n",
    "    for epoch_i in range(1, epochs+1):\n",
    "        update_loss = 0\n",
    "        batch_loss = 0\n",
    "        for batch_i, (targets_batch, inputs_batch, targets_lengths, inputs_lengths) in enumerate(\n",
    "                model.get_batches(dec_targets, enc_inputs, vocab2int, batch_size)):\n",
    "            start_time = time.time()\n",
    "            _, loss = sess.run(\n",
    "                [train_op, cost],\n",
    "                {input_data: inputs_batch,\n",
    "                 targets: targets_batch,\n",
    "                 lr: learning_rate,\n",
    "                 target_length: targets_lengths,\n",
    "                 input_length: inputs_lengths,\n",
    "                 keep_prob: keep_probability})\n",
    "\n",
    "            batch_loss += loss\n",
    "            update_loss += loss\n",
    "            end_time = time.time()\n",
    "            batch_time = end_time - start_time\n",
    "\n",
    "            if batch_i % display_step == 0:\n",
    "                print('Epoch {:>3}/{} Batch {:>4}/{} - Loss: {:>6.3f}, Seconds: {:>4.2f}'\n",
    "                      .format(epoch_i,\n",
    "                              epochs, \n",
    "                              batch_i, \n",
    "                              len(enc_inputs) // batch_size, \n",
    "                              batch_loss / display_step, \n",
    "                              batch_time*display_step))\n",
    "                batch_loss = 0\n",
    "\n",
    "            #print (batch_i)\n",
    "            if batch_i % (update_check) == 0 and batch_i > 0:\n",
    "                print(\"Mean loss for this update:\", round(update_loss/update_check, 3))\n",
    "                question_update_loss.append(update_loss)\n",
    "                \n",
    "                # If the update loss is at a new minimum, save the model\n",
    "                if update_loss <= min(question_update_loss):\n",
    "                    stop_early = 0\n",
    "                    saver.save(sess, checkpoint_path)\n",
    "\n",
    "                else:\n",
    "                    stop_early += 1\n",
    "                    if stop_early == stop:\n",
    "                        break\n",
    "                update_loss = 0\n",
    "            \n",
    "        # Reduce learning rate, but not below its minimum value\n",
    "        learning_rate *= learning_rate_decay\n",
    "        if learning_rate < min_learning_rate:\n",
    "            learning_rate = min_learning_rate\n",
    "        \n",
    "        if stop_early == stop:\n",
    "            print(\"Stopping Training since minimum loss is achieved\")\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cleanText(text, replace_entities=True):\n",
    "    \"\"\"Cleans the text in the same way as in data preprocessing part before training\"\"\"\n",
    "    if replace_entities:\n",
    "        spacy_text = nlp(text)\n",
    "        text_ents = [(str(ent), str(ent.label_)) for ent in spacy_text.ents]\n",
    "        \n",
    "        text = text.lower()\n",
    "        # Replace entities\n",
    "        for ent in text_ents:\n",
    "            replacee = str(ent[0].lower())\n",
    "            replacer = str(ent[1])\n",
    "            try:\n",
    "                text = text.replace(replacee, replacer)\n",
    "            except:\n",
    "                pass\n",
    "    else:\n",
    "        text = text.lower()\n",
    "        \n",
    "    spacy_text = nlp(text)\n",
    "    spacy_text = [str(token.orth_) for token in spacy_text \n",
    "                  if not token.is_punct and not token.is_stop]\n",
    "    spacy_text = ' '.join(spacy_text)\n",
    "\n",
    "    return spacy_text\n",
    "        \n",
    "def text2seq(input_sequence):\n",
    "    \"\"\"Prepare the text for the model\"\"\"\n",
    "    text = cleanText(input_sequence)\n",
    "    return [vocab2int.get(word, vocab2int['<UNK>']) for word in text.split()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "int2vocab = loadpickle('int2vocab')\n",
    "vocab2int = loadpickle('vocab2int')\n",
    "dev_squad_paragraphs = loadpickle('dev_squad_paragraphs')\n",
    "dev_squad_paragraphs = list(set(dev_squad_paragraphs))\n",
    "\n",
    "random_example = randint(0, len(dev_squad_paragraphs))\n",
    "input_sequence = dev_squad_paragraphs[random_example]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting parameters same as that of training\n",
    "epochs = 100\n",
    "batch_size = 32\n",
    "rnn_size = 512\n",
    "num_layers = 2\n",
    "learning_rate = 0.005\n",
    "keep_probability = 0.75     \n",
    "beam_width = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Restoring old model from ckpt/model.ckpt...\n",
      "INFO:tensorflow:Restoring parameters from ckpt/model.ckpt\n"
     ]
    }
   ],
   "source": [
    "text = text2seq(input_sequence)\n",
    "checkpoint_path = 'ckpt/model.ckpt'\n",
    "\n",
    "loaded_graph = tf.Graph()\n",
    "with tf.Session(graph=loaded_graph) as sess:\n",
    "    # Load saved model\n",
    "    try:\n",
    "        print('Restoring old model from %s...' % checkpoint_path)\n",
    "        loader = tf.train.import_meta_graph(checkpoint_path + '.meta')\n",
    "        loader.restore(sess, checkpoint_path)\n",
    "    except: \n",
    "        raise 'Checkpoint directory not found!'\n",
    "\n",
    "    input_data = loaded_graph.get_tensor_by_name('input:0')\n",
    "    logits = loaded_graph.get_tensor_by_name('predictions:0')\n",
    "    input_length = loaded_graph.get_tensor_by_name('input_length:0')\n",
    "    target_length = loaded_graph.get_tensor_by_name('target_length:0')\n",
    "    keep_prob = loaded_graph.get_tensor_by_name('keep_prob:0')\n",
    "    \n",
    "    #Multiply by batch_size to match the model's input parameters\n",
    "    answer_logits = sess.run(logits, {input_data: [text]*batch_size, \n",
    "                                      target_length: [25], \n",
    "                                      input_length: [len(text)]*batch_size,\n",
    "                                      keep_prob: 1.0})\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Text: b'What was the toughest decision you ever had to make?   When I was in the 12th standard, I was asked to choose between Biology or Statistics. We had an orientation about each of the streams; and of-course, biology(being biology) seemed way more difficult and rich a subject than statistics. I remember comparing the sizes of the books in both sections. Up until the orientation, I had always wanted to be a cardio-surgeon. My parents are both engineering profesors and so they were nudging me towards medicine. But I chose statistics, because it seemed like the easy way. It was one of the hardest decisions I have had to make.'\n",
      "\n",
      "Generated Questions:\n",
      " -- : do you regret with your choice <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS>\n",
      " -- : do you succumb with your choice <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS>\n",
      " -- : was this pure of your choice <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS>\n"
     ]
    }
   ],
   "source": [
    "# Removing the padding\n",
    "pad = vocab2int[\"<PAD>\"] \n",
    "new_logits = []\n",
    "for i in range(batch_size):\n",
    "    new_logits.append(answer_logits[i].T)\n",
    "\n",
    "print('Original Text:', input_sequence.encode('utf-8').strip())\n",
    "\n",
    "print('\\nGenerated Questions:')\n",
    "for index in range(beam_width):\n",
    "    print(' -- : {}'.format(\" \".join([int2vocab[i] for i in new_logits[1][index] if i != pad and i != -1])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Text: b'What is the difference between hard work and smart work? Hard work and smart work are entirely different things. Although they are independent things, having existence on their own, smart work can result from hard work. So the major difference is that even though someone works really hard on something, there is necessarily no need that his/her resultant work should be something worth considering smart. Smart work comes as a result of careful and intelligent thinking followed by precise application of those ideas. This might involve a lot of hard work even though it is not necessary.'\n",
      "\n",
      "Generated Questions:\n",
      " -- : what are you saying there is no distinction between your life <EOS> <EOS> <EOS> <EOS> <EOS>\n",
      " -- : what are you saying there is no distinction between them <EOS> <EOS> <EOS> <EOS> <EOS> <EOS>\n",
      " -- : how are you saying there is no distinction between your life <EOS> <EOS> <EOS> <EOS> <EOS>\n"
     ]
    }
   ],
   "source": [
    "# Removing the padding\n",
    "pad = vocab2int[\"<PAD>\"] \n",
    "new_logits = []\n",
    "for i in range(batch_size):\n",
    "    new_logits.append(answer_logits[i].T)\n",
    "\n",
    "print('Original Text:', input_sequence.encode('utf-8').strip())\n",
    "\n",
    "print('\\nGenerated Questions:')\n",
    "for index in range(beam_width):\n",
    "    print(' -- : {}'.format(\" \".join([int2vocab[i] for i in new_logits[1][index] if i != pad and i != -1])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Text: b'If you could relive the last 10 years of your life, what would you do differently? I am happy with what my life has turned out to be. I would not make any big changes but may be change small small things like be more proactive and be helpful and polite.'\n",
      "\n",
      "Generated Questions:\n",
      " -- : how does your self motivation can play self role <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS>\n",
      " -- : how does your self motivation <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS>\n",
      " -- : how does your self motivation can play your role <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS>\n"
     ]
    }
   ],
   "source": [
    "# Removing the padding\n",
    "pad = vocab2int[\"<PAD>\"] \n",
    "new_logits = []\n",
    "for i in range(batch_size):\n",
    "    new_logits.append(answer_logits[i].T)\n",
    "\n",
    "print('Original Text:', input_sequence.encode('utf-8').strip())\n",
    "\n",
    "print('\\nGenerated Questions:')\n",
    "for index in range(beam_width):\n",
    "    print(' -- : {}'.format(\" \".join([int2vocab[i] for i in new_logits[1][index] if i != pad and i != -1])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Text: b'What are your passions? Apart from my studies, i want to visit new places in the world. I also have a diehard feeling to go on a roadtrip in spain just like the movie zindigi na milegi dubaara. \\n\\\\\\nI also wish to develop games and other software in computer science. Apart from these i also want to continue with building various others stuffs like the water level sensor i discussed about in my intro. thses electronic stuff fascinate me alot!!'\n",
      "\n",
      "Generated Questions:\n",
      " -- : how strong related your machine <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS>\n",
      " -- : how have you identify creativity related <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS>\n",
      " -- : what strong related your machine <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS>\n"
     ]
    }
   ],
   "source": [
    "# Removing the padding\n",
    "pad = vocab2int[\"<PAD>\"] \n",
    "new_logits = []\n",
    "for i in range(batch_size):\n",
    "    new_logits.append(answer_logits[i].T)\n",
    "\n",
    "print('Original Text:', input_sequence.encode('utf-8').strip())\n",
    "\n",
    "print('\\nGenerated Questions:')\n",
    "for index in range(beam_width):\n",
    "    print(' -- : {}'.format(\" \".join([int2vocab[i] for i in new_logits[1][index] if i != pad and i != -1])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
